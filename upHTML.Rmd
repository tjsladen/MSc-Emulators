---
title: "optimal value acquisition"
author: "TSladen"
date: "2024-06-06"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
# Optimal Value Acquisition

## setting up data
Uses mean value for each year ie for 1850 a value of 59.8 is used for all observations
```{r}
library(tidyverse)
library(ggplot2)
library(reshape2)
library(DiceKriging)
library(rBayesianOptimization)
load("C:\\Users\\tjsla\\OneDrive\\Desktop\\Masters\\Dissertation\\ensemble-jules-historical-timeseries.RData")

y1TR<- baresoilFrac_lnd_mean_ens_wave00[1:399, ]
y1TS<- baresoilFrac_lnd_mean_ens_wave00[400:499, ]

y2TR<- c3PftFrac_lnd_mean_ens_wave00[1:399, ]
y2TS<- c3PftFrac_lnd_mean_ens_wave00[400:499, ]

y3TR<- c4PftFrac_lnd_mean_ens_wave00[1:399, ]
y3TS<- c4PftFrac_lnd_mean_ens_wave00[400:499, ]

y4TR<- cSoil_ens_wave00[1:399, ]
y4TS<- cSoil_ens_wave00[400:499, ]

y5TR<- cVeg_ens_wave00[1:399, ]
y5TS<- cVeg_ens_wave00[400:499, ]

y6TR<- fHarvest_lnd_sum_ens_wave00[1:399, ]
y6TS<- fHarvest_lnd_sum_ens_wave00[400:499, ]

y7TR<- fLuc_lnd_sum_ens_wave00[1:399, ]
y7TS<- fLuc_lnd_sum_ens_wave00[400:499, ]

y8TR<- lai_lnd_mean_ens_wave00[1:399, ]
y8TS<- lai_lnd_mean_ens_wave00[400:499, ]

y9TR<- nbp_ens_wave00[1:399, ]
y9TS<- nbp_ens_wave00[400:499, ]

y10TR<- npp_ens_wave00[1:399, ]
y10TS<- npp_ens_wave00[400:499, ]

y11TR<- rh_lnd_sum_ens_wave00[1:399, ]
y11TS<- rh_lnd_sum_ens_wave00[400:499, ]

y12TR<- shrubFrac_lnd_mean_ens_wave00[1:399, ]
y12TS<- shrubFrac_lnd_mean_ens_wave00[400:499, ]

y13TR<- treeFrac_lnd_mean_ens_wave00[1:399, ]
y13TS<- treeFrac_lnd_mean_ens_wave00[400:499, ]

yTRL<- list(y1TR,y2TR,y3TR,y4TR,y5TR,y6TR,y7TR,y8TR,y9TR,y10TR,y11TR,y12TR,y13TR)
yTSL<- list(y1TS,y2TS,y3TS,y4TS,y5TS,y6TS,y7TS,y8TS,y9TS,y10TS,y11TS,y12TS,y13TS)

x_train<- X[1:399, ]
x_test<- X[400:499, ]

calculate_meansTR <- function(df_list) {
  meanL <- list()
  
  for (i in seq_along(df_list)) {
    df <- df_list[[i]]
    
    Means <- colMeans(df, na.rm = TRUE)
    Means <- matrix(Means, nrow = 1, ncol = length(Means))
    Means_rep<- rep(Means, each = 399)
    Means_F<- matrix(Means_rep, nrow = 399, ncol = length(Means), byrow = F)
    colnames(Means_F)<- years
    meanL[[i]] <- Means_F
  }
  return(meanL)
}

calculate_meansTS <- function(df_list) {
  meanL <- list()
  
  for (i in seq_along(df_list)) {
    df <- df_list[[i]]
    
    Means <- colMeans(df, na.rm = TRUE)
    Means <- matrix(Means, nrow = 1, ncol = length(Means))
    Means_rep<- rep(Means, each = 100)
    Means_F<- matrix(Means_rep, nrow = 100, ncol = length(Means), byrow = F)
    colnames(Means_F)<- years
    meanL[[i]] <- Means_F
  }
  return(meanL)
}

outputTR<- calculate_meansTR(yTRL)
outputTS<- calculate_meansTS(yTSL)

outputTR20<- lapply(outputTR, function(matrix) matrix[, 1:20])
outputTS20<- lapply(outputTS, function(matrix) matrix[, 1:20])

y_train1<- outputTR20[[1]]
y_train2<- outputTR20[[2]]
y_train3<- outputTR20[[3]]
y_train4<- outputTR20[[4]]
y_train5<- outputTR20[[5]]
y_train6<- outputTR20[[6]]
y_train7<- outputTR20[[7]]
y_train8<- outputTR20[[8]]
y_train9<- outputTR20[[9]]
y_train10<- outputTR20[[10]]
y_train11<- outputTR20[[11]]
y_train12<- outputTR20[[12]]
y_train13<- outputTR20[[13]]
y_test1<- outputTS20[[1]]
y_test2<- outputTS20[[2]]
y_test3<- outputTS20[[3]]
y_test4<- outputTS20[[4]]
y_test5<- outputTS20[[5]]
y_test6<- outputTS20[[6]]
y_test7<- outputTS20[[7]]
y_test8<- outputTS20[[8]]
y_test9<- outputTS20[[9]]
y_test10<- outputTS20[[10]]
y_test11<- outputTS20[[11]]
y_test12<- outputTS20[[12]]
y_test13<- outputTS20[[13]]
```
## Setup functions for training emulators
Various functions, starts with training emulators -> makes predictions then extracts useful metrics for evaluation of models

```{r}
train_emulators <- function(y_train, x_train) {
  gp_models <- list()
  
  for (i in 1:ncol(y_train)) {
    gp_model <- km(design = x_train, response = y_train[, i], nugget.estim = T,
                   control = list(trace = FALSE))
    gp_models[[i]] <- gp_model
  }
  return(gp_models)
}

make_predictions <- function(gp_models, x_test) {
  predictions <- lapply(gp_models, function(model) predict.km(model, newdata = x_test, type = "UK"))
  return(predictions)
}

calculate_rmse <- function(predictions, actuals) {
  sqrt(mean((predictions$mean - actuals)^2, na.rm = TRUE))
}

extract_hyperparameters <- function(model) {
  list(
    length_scales = model@covariance@range.val,
    nugget = model@covariance@nugget,
    variance = model@covariance@sd2
  )
}

extract_hyperparameters_from_list <- function(model_list) {
  hyperparameters_list <- lapply(model_list, extract_hyperparameters)
  hyperparameters_df <- do.call(rbind, lapply(hyperparameters_list, as.data.frame))
  
  hyperparameters_df$model_index <- rep(1:length(model_list), each = nrow(hyperparameters_df) / length(model_list))
  
  hyperparameters_df <- hyperparameters_df[, c("model_index", setdiff(names(hyperparameters_df), "model_index"))]
  
  return(hyperparameters_df)
}

extract_predictions_to_df <- function(predictions, x_test) {
  n_models <- length(predictions)
  n_samples <- nrow(x_test)
  
  results <- data.frame(
    model_index = rep(1:n_models, each = n_samples),
    test_sample_index = rep(1:n_samples, times = n_models),
    predicted_mean = unlist(lapply(predictions, function(p) p$mean)),
    predicted_variance = unlist(lapply(predictions, function(p) p$sd^2))
  )
  
  return(results)
}
```
## Runing emulator for baresoilfrac
runs the emulator for the first variable in this case baresoilfrac for years 1850 to 1869, providing error metrics and extraction of hyperparameters alongside average prediction and associated variance for each year.
```{r}
gp_models_y1 <- train_emulators(y_train1, x_train)
predictions_y1 <- make_predictions(gp_models_y1, x_test)
rmse_values1 <- numeric(length(predictions_y1))
for(i in seq_along(predictions_y1)){
  rmse_values1[i]<- calculate_rmse(predictions_y1[[i]], y_test1)
}
print(rmse_values1)
mean_observed1 <- apply(y_test1, 2, mean)
percentage_error1 <- (rmse_values1 / mean_observed1) * 100
print(percentage_error1)
print(min(percentage_error1))
print(max(percentage_error1))
hp_y1<- extract_hyperparameters_from_list(gp_models_y1)
predictions_df_y1 <- extract_predictions_to_df(predictions_y1, x_test)
average_predictions1 <- aggregate(cbind(predicted_mean, predicted_variance) ~ model_index, data = predictions_df_y1, FUN = mean)
print(average_predictions1)
```
## further metrics
Shows some more metrics for evaluation in this case MAE and some residual plots.

```{r}
calculate_mae <- function(predictions_list, actuals) {
  mae_values <- numeric(length(predictions_list))
  for (i in seq_along(predictions_list)) {
    mae_values[i] <- mean(abs(predictions_list[[i]]$mean - actuals[, i]), na.rm = TRUE)
  }
  return(mae_values)
}

mae_values <- calculate_mae(predictions_y1, y_test1)
print(mae_values)

calculate_mae_percentage <- function(predictions_list, actuals) {
  mae_values <- numeric(length(predictions_list))
  for (i in seq_along(predictions_list)) {
    mae_values[i] <- mean(abs(predictions_list[[i]]$mean - actuals[, i]), na.rm = TRUE)
  }
  mean_observed <- apply(actuals, 2, mean)
  mae_percentage <- (mae_values / mean_observed) * 100
  return(mae_percentage)
}

mae_percentage_values <- calculate_mae_percentage(predictions_y1, y_test1)
print(mae_percentage_values)

visualise_residuals <- function(predictions_list, actuals) {
  # Initialize a list to store residual plots
  residual_plots <- list()
  
  # Loop through each model in the predictions list
  for (i in seq_along(predictions_list)) {
    # Extract predicted values
    predictions <- predictions_list[[i]]$mean
    
    # Calculate residuals
    residuals <- actuals[, i] - predictions
    
    # Create a residual plot
    plot(residuals, 
         main = paste("Residuals for Model", i), 
         xlab = "Observation Index", 
         ylab = "Residuals")
    
    # Add a horizontal line at y = 0 for reference
    abline(h = 0, col = "red")
    
    # Store the plot in the list
    residual_plots[[i]] <- list(plot = recordPlot(), residuals = residuals)
  }
  
  return(residual_plots)
}

residual_plots <- visualise_residuals(predictions_y1, y_test1)

plot_predictions_vs_actuals <- function(predictions_list, actuals) {
  # Initialize a list to store the plots
  plots <- list()
  
  # Loop through each model in the predictions list
  for (i in seq_along(predictions_list)) {
    # Extract predicted values
    predictions <- predictions_list[[i]]$mean
    
    # Create a scatter plot of actual vs. predicted values
    plot(actuals[, i], predictions,
         main = paste("Predictions vs. Actuals for Model", i),
         xlab = "Actual Values",
         ylab = "Predicted Values",
         pch = 16,  # Use solid dots for points
         col = "blue")  # Use blue color for points
    
    # Add a diagonal reference line
    abline(a = 0, b = 1, col = "red")
    
    # Store the plot in the list
    plots[[i]] <- recordPlot()
  }
  
  return(plots)
}

prediction_actual_plots <- plot_predictions_vs_actuals(predictions_y1, y_test1)
```
## Sensitivity analysis
Initial analysis was based on 1 model form the list later updated to be able to run for all models (higher cost). Provides suggested values for use in the bayesian optimisation process. Plots visualise impact of hyperparameter changes on models rmse.

```{r}
sensitivity_analysis <- function(gp_model, x_train, y_train, x_test, y_test, hyperparameter, initial_values, factor = 2, n_steps = 10) {
  results <- data.frame()
  
  if (hyperparameter == "length_scale") {
    for (i in seq_along(initial_values)) {
      values <- seq(initial_values[i] / factor, initial_values[i] * factor, length.out = n_steps)
      for (value in values) {
        modified_model <- gp_model
        modified_model@covariance@range.val[i] <- value
        
        # Refit the model with modified length scale
        refitted_model <- km(
          design = x_train,
          response = y_train,
          nugget = modified_model@covariance@nugget,
          covtype = "matern5_2",
          coef.cov = modified_model@covariance@range.val,
          coef.var = modified_model@covariance@sd2,
          control = list(trace = FALSE)
        )
        
        # Make predictions on the test set
        predictions <- predict.km(refitted_model, newdata = x_test, type = "UK")
        
        # Calculate RMSE
        rmse_value <- sqrt(mean((predictions$mean - y_test)^2, na.rm = TRUE))
        
        # Store results
        results <- rbind(results, data.frame(hyperparameter = paste0("length_scale_", i), hyperparameter_value = value, rmse = rmse_value))
      }
    }
  } else {
    values <- seq(initial_values / factor, initial_values * factor, length.out = n_steps)
    for (value in values) {
      modified_model <- gp_model
      
      if (hyperparameter == "nugget") {
        modified_model@covariance@nugget <- value
      } else if (hyperparameter == "variance") {
        modified_model@covariance@sd2 <- value
      }
      
      # Refit the model with modified hyperparameter
      refitted_model <- km(
        design = x_train,
        response = y_train,
        nugget = modified_model@covariance@nugget,
        covtype = "matern5_2",
        coef.cov = modified_model@covariance@range.val,
        coef.var = modified_model@covariance@sd2,
        control = list(trace = FALSE)
      )
      
      # Make predictions on the test set
      predictions <- predict.km(refitted_model, newdata = x_test, type = "UK")
      
      # Calculate RMSE
      rmse_value <- sqrt(mean((predictions$mean - y_test)^2, na.rm = TRUE))
      
      # Store results
      results <- rbind(results, data.frame(hyperparameter = hyperparameter, hyperparameter_value = value, rmse = rmse_value))
    }
  }
  
  return(results)
}
plot_sensitivity <- function(results, hyperparameter_name) {
  ggplot(results, aes(x = hyperparameter_value, y = rmse)) +
    geom_line() +
    geom_point() +
    labs(title = paste("Sensitivity Analysis for", hyperparameter_name),
         x = hyperparameter_name,
         y = "RMSE") +
    theme_minimal()
}
gp_modely1_1 <- gp_models_y1[[1]]
y_train1_1 <- y_train1[, 1]
y_test1_1 <- y_test1[, 1]

initial_length_scale1_1 <- hp_y1$length_scales[hp_y1$model_index == 1]
initial_nugget1_1 <- hp_y1$nugget[hp_y1$model_index == 1][1]
initial_variance1_1 <- hp_y1$variance[hp_y1$model_index == 1][1]

length_scale_resultsy1_1 <- sensitivity_analysis(gp_modely1_1, x_train, y_train1_1, x_test, y_test1_1, "length_scale", initial_length_scale1_1)
nugget_resultsy1_1 <- sensitivity_analysis(gp_modely1_1, x_train, y_train1_1, x_test, y_test1_1, "nugget", initial_nugget1_1)
variance_resultsy1_1 <- sensitivity_analysis(gp_modely1_1, x_train, y_train1_1, x_test, y_test1_1, "variance", initial_variance1_1)

plot_sensitivity(length_scale_resultsy1_1, "Length Scale")
plot_sensitivity(nugget_resultsy1_1, "Nugget")
plot_sensitivity(variance_resultsy1_1, "Variance")
```
## Sensitivity applied to all

```{r}
sensitivity_analysis_all_models <- function(gp_models, x_train, y_train, x_test, y_test, hyperparameter, initial_values_list, factor = 2, n_steps = 10) {
  results_all <- data.frame()
  
  for (model_index in seq_along(gp_models)) {
    gp_model <- gp_models[[model_index]]
    y_train_model <- y_train[, model_index]
    y_test_model <- y_test[, model_index]
    initial_values <- initial_values_list[[model_index]]
    
    if (hyperparameter == "length_scale") {
      for (i in seq_along(initial_values)) {
        values <- seq(initial_values[i] / factor, initial_values[i] * factor, length.out = n_steps)
        for (value in values) {
          modified_model <- gp_model
          modified_model@covariance@range.val[i] <- value
          
          # Refit the model with modified length scale
          refitted_model <- km(
            design = x_train,
            response = y_train_model,
            nugget = modified_model@covariance@nugget,
            covtype = "matern5_2",
            coef.cov = modified_model@covariance@range.val,
            coef.var = modified_model@covariance@sd2,
            control = list(trace = FALSE)
          )
          
          # Make predictions on the test set
          predictions <- predict.km(refitted_model, newdata = x_test, type = "UK")
          
          # Calculate RMSE
          rmse_value <- sqrt(mean((predictions$mean - y_test_model)^2, na.rm = TRUE))
          
          # Store results
          results_all <- rbind(results_all, data.frame(model_index = model_index, hyperparameter = paste0("length_scale_", i), hyperparameter_value = value, rmse = rmse_value))
        }
      }
    } else {
      values <- seq(initial_values / factor, initial_values * factor, length.out = n_steps)
      for (value in values) {
        modified_model <- gp_model
        
        if (hyperparameter == "nugget") {
          modified_model@covariance@nugget <- value
        } else if (hyperparameter == "variance") {
          modified_model@covariance@sd2 <- value
        }
        
        # Refit the model with modified hyperparameter
        refitted_model <- km(
          design = x_train,
          response = y_train_model,
          nugget = modified_model@covariance@nugget,
          covtype = "matern5_2",
          coef.cov = modified_model@covariance@range.val,
          coef.var = modified_model@covariance@sd2,
          control = list(trace = FALSE)
        )
        
        # Make predictions on the test set
        predictions <- predict.km(refitted_model, newdata = x_test, type = "UK")
        
        # Calculate RMSE
        rmse_value <- sqrt(mean((predictions$mean - y_test_model)^2, na.rm = TRUE))
        
        # Store results
        results_all <- rbind(results_all, data.frame(model_index = model_index, hyperparameter = hyperparameter, hyperparameter_value = value, rmse = rmse_value))
      }
    }
  }
  
  return(results_all)
}

plot_sensitivity_all_models <- function(results, hyperparameter_name) {
  ggplot(results, aes(x = hyperparameter_value, y = rmse, color = factor(model_index))) +
    geom_line() +
    geom_point() +
    labs(title = paste("Sensitivity Analysis for", hyperparameter_name),
         x = hyperparameter_name,
         y = "RMSE") +
    theme_minimal() +
    facet_wrap(~ model_index)
}

initial_length_scales <- lapply(seq_along(gp_models_y1), function(i) hp_y1$length_scales[hp_y1$model_index == i])
initial_nuggets <- lapply(seq_along(gp_models_y1), function(i) hp_y1$nugget[hp_y1$model_index == i][1])
initial_variances <- lapply(seq_along(gp_models_y1), function(i) hp_y1$variance[hp_y1$model_index == i][1])

length_scale_results_all <- sensitivity_analysis_all_models(gp_models_y1, x_train, y_train1, x_test, y_test1, "length_scale", initial_length_scales)
nugget_results_all <- sensitivity_analysis_all_models(gp_models_y1, x_train, y_train1, x_test, y_test1, "nugget", initial_nuggets)
variance_results_all <- sensitivity_analysis_all_models(gp_models_y1, x_train, y_train1, x_test, y_test1, "variance", initial_variances)

plot_sensitivity_all_models(length_scale_results_all, "Length Scale")
plot_sensitivity_all_models(nugget_results_all, "Nugget")
plot_sensitivity_all_models(variance_results_all, "Variance")
```
## Bayesian optimisation
Using values from the sensitivity analysis, determines the best performing parameter values which can be re-inputted into the gp model training, allows for process repetition until lowest error given. Sensitivity values would replace current values in 'bounds' list.

```{r}
bayes_opt_function <- function(lengthscale, nugget, variance) {
  set.seed(123)
  
  # Create a copy of the model list to modify
  gp_models <- lapply(1:length(gp_models_y1), function(i) {
    model <- gp_models_y1[[i]]
    model@covariance@range.val <- rep(lengthscale, length(model@covariance@range.val))
    model@covariance@nugget <- nugget
    model@covariance@sd2 <- variance
    return(model)
  })
  
  # Calculate the RMSE for the modified models
  rmse_values <- numeric(length(gp_models))
  for (i in seq_along(gp_models)) {
    predictions <- predict(gp_models[[i]], newdata = x_test, type = "UK")
    rmse_values[i] <- calculate_rmse(predictions, y_test1[, i])
  }
  
  # Return the negative mean RMSE as the objective (because the optimizer maximizes by default)
  return(list(Score = -mean(rmse_values)))
}

bounds <- list(
  lengthscale = c(0.01, 10),
  nugget = c(1e-10, 1),
  variance = c(0.1, 10))

bayes_opt <- BayesianOptimization(
  FUN = bayes_opt_function,
  bounds = bounds,
  init_points = 10, # Number of initial random searches
  n_iter = 30,     # Number of optimization iterations
  acq = "ucb",     # Acquisition function (Upper Confidence Bound)
  kappa = 2.576,   # Exploration parameter for UCB
  verbose = F)

print(bayes_opt$Best_Par)
```